{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c90cb5b64c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;31m# data I/O\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# should be simple plain text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_input.txt'"
     ]
    }
   ],
   "source": [
    "# %load rnn.py\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "\n",
    "  def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "      xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "      xs[t][inputs[t]] = 1\n",
    "      hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "      ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "      ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "      loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "      dy = np.copy(ps[t])\n",
    "      dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "      dWhy += np.dot(dy, hs[t].T)\n",
    "      dby += dy\n",
    "      dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "      dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "      dbh += dhraw\n",
    "      dWxh += np.dot(dhraw, xs[t].T)\n",
    "      dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "      dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "      np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "  def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "      h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "      y = np.dot(Why, h) + by\n",
    "      p = np.exp(y) / np.sum(np.exp(y))\n",
    "      ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "      x = np.zeros((vocab_size, 1))\n",
    "      x[ix] = 1\n",
    "      ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "\n",
    "  def sample_top3(h, seed_ix):\n",
    "    \"\"\" \n",
    "    output the top3 probable next letters \n",
    "    h is memory state, seed_ix is seed letter\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ixes = np.random.choice(range(vocab_size),3 , p=p.ravel())\n",
    "    return ixes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # data I/O\n",
    "  data = open('./example/train_input.txt', 'r').read() # should be simple plain text file\n",
    "  chars = list(set(data))\n",
    "  data_size, vocab_size = len(data), len(chars)\n",
    "  print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "  char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "  ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "  # hyperparameters\n",
    "  hidden_size = 100 # size of hidden layer of neurons\n",
    "  seq_length = 25 # number of steps to unroll the RNN for\n",
    "  learning_rate = 1e-1\n",
    "\n",
    "  # model parameters\n",
    "  Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "  Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "  Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "  bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "  by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "  n, p = 0, 0\n",
    "  mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "  smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "  while n<=100000:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "      hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "      p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = Model.lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "      mem += dparam * dparam\n",
    "      param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  f_pred = open(\"pred.txt\", \"w\")\n",
    "\n",
    "  with open('test_input.txt') as f:\n",
    "    for line in f:\n",
    "      line = line.split()\n",
    "      char = list(line[len(line)-1])\n",
    "      i = char[len(char)-1] #get last character of input line\n",
    "      sample_ix = Model.sample_top3(hprev, char_to_ix[i])\n",
    "      txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "      f_pred.write(txt + '\\n')\n",
    "  f_pred.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
