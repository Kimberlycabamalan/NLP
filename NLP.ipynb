{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvk-_qkULZWc"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature=temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            indices = skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                               return_state=True)\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits/self.temperature\n",
        "        # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "def write_pred(preds, fname):\n",
        "    with open(fname, 'wt') as f:\n",
        "        for p in preds:\n",
        "            p = p.replace(\"\\n\", \"\")\n",
        "            print(p)\n",
        "            f.write('{}\\n'.format(p))\n",
        "\n",
        "def load_test_data(fname):\n",
        "    data = []\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            inp = line[:-1]  # the last character is a newline\n",
        "            data.append(inp)\n",
        "    return data\n",
        "\n",
        "def save(vocab, chars_from_ids, work_dir):\n",
        "    with open(os.path.join(work_dir, 'model.checkpoint.vocab'), 'wt') as f:\n",
        "        for val in vocab:\n",
        "            f.write(val + \"\\n\")\n",
        "        f.write(str(len(vocab)))\n",
        "\n",
        "def load(work_dir):\n",
        "    data = []\n",
        "    with open(os.path.join(work_dir, 'model.checkpoint.vocab')) as f:\n",
        "        f = list(line)\n",
        "        for line in f[:-1]:\n",
        "            line = line.split()  # the last character is a newline\n",
        "            data.append(line[0])\n",
        "        vocab_size = f[-1]\n",
        "    return data, vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8yfl1GGMR7i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7BzfTOLk7q"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    mode = \"train\"\n",
        "    dir = \"drive/MyDrive/Colab/NLP/\"\n",
        "    work_dir = dir + \"work\"\n",
        "    train_data = dir + \"data/training.txt\"\n",
        "\n",
        "    random.seed(0)\n",
        "    work_dir\n",
        "\n",
        "    if mode == 'train':\n",
        "        start = time.time()\n",
        "        if not os.path.isdir(work_dir):\n",
        "            print('Making working directory {}'.format(work_dir))\n",
        "            os.makedirs(work_dir)\n",
        "\n",
        "        path_to_file = train_data\n",
        "\n",
        "        # Read, then decode for py2 compat.\n",
        "        text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "        # length of text is the number of characters in it\n",
        "        print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "        # The unique characters in the file\n",
        "        vocab = sorted(set(text))\n",
        "        vocab.remove(\"\\n\")\n",
        "        # vocab.remove(\" \")\n",
        "        print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "        # Length of the vocabulary in chars\n",
        "        vocab_size = len(vocab)\n",
        "\n",
        "        # The embedding dimension\n",
        "        embedding_dim = 256\n",
        "\n",
        "        # Number of RNN units\n",
        "        rnn_units = 1024\n",
        "\n",
        "        example_texts = ['abcdefg', 'xyz']\n",
        "        chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "        ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "\n",
        "        ids = ids_from_chars(chars)\n",
        "\n",
        "        chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "        chars = chars_from_ids(ids)\n",
        "        tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "\n",
        "        all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "        ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "        # for ids in ids_dataset.take(10):\n",
        "        # \tprint(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "        seq_length = 100\n",
        "        examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "        sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "        dataset = sequences.map(split_input_target)\n",
        "\n",
        "        # Batch size\n",
        "        BATCH_SIZE = 64\n",
        "\n",
        "        # Buffer size to shuffle the dataset\n",
        "        # (TF data is designed to work with possibly infinite sequences,\n",
        "        # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "        # it maintains a buffer in which it shuffles elements).\n",
        "        BUFFER_SIZE = 10000\n",
        "\n",
        "        dataset = (\n",
        "            dataset\n",
        "                .shuffle(BUFFER_SIZE)\n",
        "                .batch(BATCH_SIZE, drop_remainder=False)\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "        model = MyModel(\n",
        "            # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "            vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "            embedding_dim=embedding_dim,\n",
        "            rnn_units=rnn_units)\n",
        "\n",
        "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "        # Directory where the checkpoints will be saved\n",
        "        checkpoint_dir = './training_checkpoints'\n",
        "        # Name of the checkpoint files\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_prefix,\n",
        "            save_weights_only=True)\n",
        "\n",
        "        EPOCHS = 10\n",
        "        history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "        save(vocab, chars_from_ids, work_dir)\n",
        "\n",
        "        end = time.time()\n",
        "        print(f\"\\nRun time: {end - start}\")\n",
        "    elif mode == 'test':\n",
        "        start = time.time()\n",
        "        # Create a basic model instance\n",
        "        vocab, vocab_size = load(work_dir)\n",
        "\n",
        "        ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "        chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "        # The embedding dimension\n",
        "        embedding_dim = 256\n",
        "\n",
        "        # Number of RNN units\n",
        "        rnn_units = 1024\n",
        "\n",
        "        model = MyModel(\n",
        "            # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "            vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "            embedding_dim=embedding_dim,\n",
        "            rnn_units=rnn_units)\n",
        "\n",
        "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "        checkpoint_path = \"training_checkpoints\"\n",
        "        model.load_weights(checkpoint_path)\n",
        "        one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "        test_data = load_test_data('example/input.txt')\n",
        "        pred = []\n",
        "        next_char = tf.constant(test_data)\n",
        "\n",
        "        first_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "        second_choice, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "        third_choice, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "        print('1', first_char)\n",
        "        print('2', second_choice)\n",
        "        print('3', third_choice)\n",
        "        print('\\n')\n",
        "        print(len(test_data))\n",
        "        for i in range(0, len(test_data)):\n",
        "            next_char_3 = first_char[i].numpy().decode('utf-8') + second_choice[i].numpy().decode('utf-8') + third_choice[i].numpy().decode('utf-8')\n",
        "            pred.append(next_char_3)\n",
        "        write_pred(pred, 'output.txt')\n",
        "        end = time.time()\n",
        "        print(f\"\\nRun time: {end - start}\")\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError('Unknown mode {}'.format(mode))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ega5QJSMz7u"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}